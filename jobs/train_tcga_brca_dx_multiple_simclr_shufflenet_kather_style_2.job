#!/bin/bash
#SBATCH --job-name=finetune_brca
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=6
#SBATCH --partition=gpu_titanrtx_shared
#SBATCH --gres=gpu:1
#SBATCH --ntasks=1
#SBATCH --time=2:20:00
#SBATCH --array=1-5
# moved into jobs/msi dir, nothing should change besides this

#rsync -a -progress=info2 --exclude json --files-from= . /scratch/
INCLUDEFILE=/home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/metadata/tcga/crc_dx/train_paths.txt

module purge
module load 2019

module load Anaconda3
module load CUDA/10.0.130
module load cuDNN/7.6.3-CUDA-10.0.130
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.6.3-CUDA-10.0.130/lib64:$LD_LIBRARY_PATH

CONDA_PREFIX=$(conda info --base)
source $CONDA_PREFIX/etc/profile.d/conda.sh
conda deactivate
conda activate thesisp375_apex

ORIGINAL_DATA_PATH=/home/yonis/tcga_breast_ddr/ # please don't forget the trailing /. or do /home/yonis/<theone>/tiled_data_large ... without a /

cd ..
echo "starting to copy to scratch..."
date
cp /project/yonis/tcga_brca_dx/tiled_data_large.tar.gz "$TMPDIR"
#rsync -a -progress=info2 --exclude "json" $ORIGINAL_DATA_PATH "$TMPDIR"
#rsync -a -progress=info2 --exclude "json" --files-from=$INCLUDEFILE $ORIGINAL_DATA_PATH "$TMPDIR"
#rsync -ah --delete --ignore-existing --exclude "*.pt" $ORIGINAL_DATA_PATH "$TMPDIR"/
echo "done with copying to scratch..."
date
cd "$TMPDIR"
tar xzf tiled_data_large.tar.gz
echo "done with unzipping"
date
ls "$TMPDIR"
cd -
#num_workers=`expr $(nproc --all) - 1`
num_workers=4
echo "Num threads: $num_workers"

srun python3 -u -m testing.logistic_regression with config_file=./config/train-rn18-brca-dx-supervised-kather-style.yaml root_dir_for_tcga_tiles="$TMPDIR"/tiled_data_large/ kfold=$SLURM_ARRAY_TASK_ID logistic_extractor=simclr workers=$num_workers resnet=shufflenetv2_x1_0 reload_model=True model_path=./logs/pretrain/562 epoch_num=80028 feature_learning=unsupervised

#srun python3 -u -m feature_learning.main_unsupervised with feature_learning=unsupervised reload_model=False resnet=resnet18 config_file=./config/config-simclr-train-tcga-brca-kr.yaml batch_size=950 workers=$num_workers use_multi_gpu=True 
#root_dir_for_tcga_tiles="$TMPDIR"/tiled_data_large/ 
#srun python3 -u -m feature_learning.main_unsupervised with feature_learning=unsupervised reload_model=False resnet=resnet18 config_file=./config/config-simclr-train-tcga-crc-dx.yaml batch_size=128  unsupervised_method=simclr workers=$num_workers
#srun python3 -u train.py --single_gpu --name=stadlr56 --batch_size=256 --run_times=3 --epochs=100 --freeze_num=0 --evaluate_every=256 --eval_patience=3 --data=msidata/stad --save_model=True --track_tb=True --learning_rate=5e-6 --weight_decay=1e-4 --learning_rate_factor=2 --seedstart=1
