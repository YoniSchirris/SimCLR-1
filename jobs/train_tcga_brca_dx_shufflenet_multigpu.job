#!/bin/bash
#SBATCH --job-name=titan_tcga_brca_shufflenet_test_tarball
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --partition=normal
#SBATCH --ntasks=1
#SBATCH --time=3:00:00

# moved into jobs/msi dir, nothing should change besides this

#rsync -a -progress=info2 --exclude json --files-from= . /scratch/

module purge
module load 2019

module load Anaconda3
module load CUDA/10.0.130
module load cuDNN/7.6.3-CUDA-10.0.130
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.6.3-CUDA-10.0.130/lib64:$LD_LIBRARY_PATH

CONDA_PREFIX=$(conda info --base)
source $CONDA_PREFIX/etc/profile.d/conda.sh
conda activate thesisp375_apex

ORIGINAL_DATA_PATH=/home/yonis/tcga_breast_ddr/ # please don't forget the trailing /. or do /home/yonis/<theone>/tiled_data_large ... without a /

cd ..
echo "starting to copy to scratch... v-- time of starting is below"
date
#cp /project/yonis/tcga_brca_dx/tiled_data_large.tar.gz "$TMPDIR"
rsync -a /project/yonis/tcga_brca_dx/tiled_data_large.tar.gz "$TMPDIR"
#rsync -a -progress=info2 --exclude "json" $ORIGINAL_DATA_PATH "$TMPDIR"
#rsync -a -progress=info2 --exclude "json" --files-from=$INCLUDEFILE $ORIGINAL_DATA_PATH "$TMPDIR"
#rsync -ah --delete --ignore-existing --exclude "*.pt" $ORIGINAL_DATA_PATH "$TMPDIR"/
echo "done with copying to scratch... v--- time that this is finished is below"

date
cd "$TMPDIR"
tar xzf tiled_data_large.tar.gz
echo "done with unzipping v---- time that unzipping was finished is below"
date
ls "$TMPDIR"
cd -

#num_workers=`expr $(nproc --all) - 1`
num_workers=5
echo "Num threads: $num_workers"

#srun python3 -u -m feature_learning.main_unsupervised with feature_learning=unsupervised reload_model=False resnet=shufflenetv2_x1_0 config_file=./config/config-simclr-train-tcga-brca-dx.yaml batch_size=950 workers=$num_workers use_multi_gpu=True kfold=0 root_dir_for_tcga_tiles="$TMPDIR"/tiled_data_large/


#root_dir_for_tcga_tiles="$TMPDIR"/tiled_data_large/ 
#srun python3 -u -m feature_learning.main_unsupervised with feature_learning=unsupervised reload_model=False resnet=resnet18 config_file=./config/config-simclr-train-tcga-crc-dx.yaml batch_size=128  unsupervised_method=simclr workers=$num_workers
#srun python3 -u train.py --single_gpu --name=stadlr56 --batch_size=256 --run_times=3 --epochs=100 --freeze_num=0 --evaluate_every=256 --eval_patience=3 --data=msidata/stad --save_model=True --track_tb=True --learning_rate=5e-6 --weight_decay=1e-4 --learning_rate_factor=2 --seedstart=1
