# train options
seed: 42 # sacred handles automatic seeding when passed in the config
batch_size: 32
workers: 3
start_epoch: 0
epochs: 100
kfold: 1

#Data options
dataset: "msi-tcga" # STL10 / CIFAR10 / 'msi-kather' / 'mci-tga'
# if msi-kather, path_to_msi_data should be to the directory ..../msidata/crc_dx/train
# if msi-tcga, path_to_Msi_data should be to the csv file .../tcga/crc_dx/labels_for_.....csv
#path_to_msi_data: "/Users/yoni/dropbox/UvA/AI/nki/histogenomics-msc-2019/yoni-code/MsiPrediction/data/msidata/crc_dx/train/"
#path_to_test_msi_data: "/Users/yoni/dropbox/UvA/AI/nki/histogenomics-msc-2019/yoni-code/MsiPrediction/data/msidata/crc_dx/test/"
root_dir_for_tcga_tiles: "/project/yonis/tcga_brca_dx/tiled_data_large/"
path_to_msi_data: "/home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/metadata/tcga/brca_dx/data_tcga_brca_dxwith_ddr_labels_and_splits_dropna_subsample_500.csv"
path_to_test_msi_data: "/home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/metadata/tcga/crc_dx/labels_for_kather_tcga_crc_dx_train.csv"

pretrain: True 
load_patient_level_tensors: False
load_wsi_level_tensors: True
load_tensor_grid: True

data_pretrain_fraction: 1
data_testing_balance_training_data: False
data_testing_train_fraction: 1
data_testing_test_fraction: 1

data_create_feature_vectors_fraction: 1


# model options
resnet: "shufflenetv2_x1_0" # the backbone to be used by SimCLR: resnet18, resnet50, shufflenetv2_x1_0
normalize: True
projection_dim: 128 # "[...] to project the representation to a 128-dimensional latent space" only used for 

# supervised or unsupservised learning?
feature_learning: "unsupervised"
unsupervised_method: "byol" # "simclr" or "byol"
save_each_epochs: 1
projection_hidden: 128      # only used for supervised feature learning

# training on generated labels
train_extractor_on_generated_labels: False
generated_labels_id: ''
generated_label: 'label'

# loss options
optimizer: "Adam" # or LARS (experimental)
weight_decay: 1.0e-6 # "optimized using LARS [...] and weight decay of 10e-6"
temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
reload_model: False
model_path: "logs/pretrain/901" # set to the directory containing `checkpoint_##.tar` 
epoch_num: 406362 # set to checkpoint number
 
# mixed-precision training
fp16: False 
fp16_opt_level: "O2"

# testing options
logistic_extractor: 'byol' # extractor to be used during testing. either 'simclr' or 'imagenet-resnet18', 'imagenet-resnet50', or 'imagenet-shufflenetv2_x1_0'
classification_head: 'deepmil' # or 'logistic' or 'deepmil'
logistic_batch_size: 16
logistic_epochs: 4
logistic_lr: 5.0e-5
deepmil_lr: 0.0005
deepmil_reg: 10.0e-5
precompute_features: True
precompute_features_in_memory: False
use_precomputed_features: False
use_precomputed_features_id: ''
freeze_encoder: False
validation_split: 0.2
evaluate_every: 100 
ddr_label: 'median_HRD_Score' # or any of the ones defined in the ddr sheet


use_focal_loss: False
focal_loss_alpha: 1
focal_loss_gamma: 2
