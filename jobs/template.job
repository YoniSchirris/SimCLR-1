#!/bin/bash

#SBATCH --job-name=script
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=12
#SBATCH --partition=gpu_shared
#SBATCH --ntasks=1
#SBATCH --gres=gpu:1
#SBATCH --time=1-0:00:00

module purge
module load 2019

module load Anaconda3
module load CUDA/10.0.130
module load cuDNN/7.6.3-CUDA-10.0.130
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.6.3-CUDA-10.0.130/lib64:$LD_LIBRARY_PATH

CONDA_PREFIX=$(conda info --base)
source $CONDA_PREFIX/etc/profile.d/conda.sh
conda activate thesisp375

#VARIABLES
ORIGINAL_DATA_PATH=/home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/data/msidata/crc_dx/train
ORIGINAL_DATA_PATH_TEST=/home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/data/msidata/crc_dx/test
TMP_PARENT_DIR_FOR_DATA=msidata # is required for the msi dataset to know the task

cd ..
echo "starting to copy to scratch..."
date
mkdir -p "$TMPDIR"/$TMP_PARENT_DIR_FOR_DATA
rsync -ah --delete --ignore-existing --exclude "*.pt" $ORIGINAL_DATA_PATH "$TMPDIR"/$TMP_PARENT_DIR_FOR_DATA/
echo "done with copying to scratch..."
date
ls "$TMPDIR"

num_workers=$(nproc --all)
echo "Num threads: $num_workers"


srun python3 -u -m feature_learning.main_unsupervised with config_file=./config/config-byol-pretrain-rn18-crcdx.yaml batch_size=128 save_each_epochs=10 unsupervised_method=byol workers=$num_workers path_to_msi_data="$TMPDIR"/msidata/train/ fp16=True fp16_opt_level=O1
