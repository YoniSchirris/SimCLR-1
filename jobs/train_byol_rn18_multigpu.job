#!/bin/bash

#SBATCH --job-name=janne_script
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=12
#SBATCH --partition=gpu_shared
#SBATCH --ntasks=1
#SBATCH --gres=gpu:1
#SBATCH --time=1-0:00:00
#SBATCH --mem=11G

# moved into jobs/msi dir, nothing should change besides this

module purge
module load 2019

module load Anaconda3
module load CUDA/10.0.130
module load cuDNN/7.6.3-CUDA-10.0.130
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.6.3-CUDA-10.0.130/lib64:$LD_LIBRARY_PATH

CONDA_PREFIX=$(conda info --base)
source $CONDA_PREFIX/etc/profile.d/conda.sh
conda activate thesisp375

cd ..
echo "starting to copy to scratch..."
date
mkdir -p "$TMPDIR"/msidata
#cp -r /home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/data/msidata/crc_dx/train "$TMPDIR"
rsync -ah --delete --ignore-existing --exclude "*.pt" /home/yonis/histogenomics-msc-2019/yoni-code/MsiPrediction/data/msidata/crc_dx/train "$TMPDIR"/msidata/
echo "done with copying to scratch..."
date
ls "$TMPDIR"

num_workers=$(nproc --all)
echo "Num threads: $num_workers"


srun python3 -u -m feature_learning.main_unsupervised with config_file=./config/config-byol-pretrain-rn18-crcdx.yaml batch_size=128 save_each_epochs=10 unsupervised_method=byol workers=$num_workers path_to_msi_data="$TMPDIR"/msidata/train/ fp16=True fp16_opt_level=O1

#srun python3 -u train.py --single_gpu --name=stadlr56 --batch_size=256 --run_times=3 --epochs=100 --freeze_num=0 --evaluate_every=256 --eval_patience=3 --data=msidata/stad --save_model=True --track_tb=True --learning_rate=5e-6 --weight_decay=1e-4 --learning_rate_factor=2 --seedstart=1
